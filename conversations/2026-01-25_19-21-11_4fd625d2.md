---
created_at: '2026-01-25T19:21:11.450977'
current_step: 1
session_id: 4fd625d2-d558-4d42-ab51-32b0afe2e780
title: 我在写agent的时候,是否应该用langgraph,还是直...
---

## User (2026-01-25 19:21:53)
我在写agent的时候,是否应该用langgraph,还是直接写http请求?我感觉langgraph没啥用
## Assistant (2026-01-25 19:35:52)
```yaml
          else:
            echo "指标异常，回滚到0%"
            ./scripts/rollout.sh --percentage 0
            exit 1
```

## 3. 部署和运维脚本

### **Dockerfile**
```dockerfile
# Dockerfile
FROM python:3.11-slim as builder

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# 安装Poetry
RUN pip install poetry==1.5.1

# 复制依赖文件
COPY pyproject.toml poetry.lock ./

# 安装依赖（不包括开发依赖）
RUN poetry config virtualenvs.create false \
    && poetry install --no-dev --no-interaction --no-ansi

# 生产镜像
FROM python:3.11-slim

WORKDIR /app

# 安装运行时依赖
RUN apt-get update && apt-get install -y \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 复制Python依赖
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# 复制应用代码
COPY . .

# 创建非root用户
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# 启动应用
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080", "--workers", "4"]
```

### **docker-compose.yml**
```yaml
version: '3.8'

services:
  sales-analyzer:
    build: .
    image: sales-analyzer:${TAG:-latest}
    container_name: sales-analyzer
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD}@postgres:5432/analysis
      - REDIS_URL=redis://redis:6379/0
      - OTEL_EXPORTER=otlp
      - OTLP_ENDPOINT=http://tempo:4317
      - LOG_LEVEL=INFO
    ports:
      - "8080:8080"
    volumes:
      - ./data/checkpoints:/app/data/checkpoints
      - ./logs:/app/logs
    networks:
      - monitoring
      - app-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.sales-analyzer.rule=Host(`analyzer.example.com`)"
      - "traefik.http.services.sales-analyzer.loadbalancer.server.port=8080"

  postgres:
    image: postgres:15-alpine
    container_name: sales-analyzer-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=analysis
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: sales-analyzer-cache
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 监控栈
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3000:3000"
    networks:
      - monitoring

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    restart: unless-stopped
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./monitoring/tempo.yaml:/etc/tempo.yaml
      - tempo_data:/tmp/tempo
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "3200:3200"  # Tempo API
    networks:
      - monitoring

networks:
  monitoring:
    driver: bridge
  app-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  tempo_data:
```

### **Kubernetes部署配置**
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sales-analyzer
  namespace: analysis
  labels:
    app: sales-analyzer
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: sales-analyzer
  template:
    metadata:
      labels:
        app: sales-analyzer
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: sales-analyzer-sa
      containers:
      - name: analyzer
        image: ghcr.io/your-org/sales-analyzer:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: connection-string
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: connection-string
        - name: OTEL_SERVICE_NAME
          value: "sales-analyzer"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://tempo.observability:4317"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        volumeMounts:
        - name: checkpoint-storage
          mountPath: /app/data/checkpoints
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: checkpoint-storage
        persistentVolumeClaim:
          claimName: checkpoint-pvc
      - name: config-volume
        configMap:
          name: analyzer-config
---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sales-analyzer-hpa
  namespace: analysis
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sales-analyzer
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: analysis_queue_length
      target:
        type: AverageValue
        averageValue: 100
```

## 4. 运维和监控脚本

### **健康检查脚本**
```python
# scripts/health_check.py
import asyncio
import httpx
import sys
from typing import Dict, List
import json

class HealthChecker:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=10.0)
    
    async def check_health(self) -> Dict:
        """执行完整的健康检查"""
        checks = {
            "api": self.check_api,
            "database": self.check_database,
            "redis": self.check_redis,
            "checkpoints": self.check_checkpoints,
            "graph": self.check_graph,
            "memory": self.check_memory
        }
        
        results = {}
        for name, check_func in checks.items():
            try:
                results[name] = await check_func()
            except Exception as e:
                results[name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # 总体状态
        all_healthy = all(r.get("status") == "healthy" for r in results.values())
        results["overall"] = {
            "status": "healthy" if all_healthy else "unhealthy",
            "timestamp": datetime.now().isoformat()
        }
        
        return results
    
    async def check_api(self) -> Dict:
        """检查API端点"""
        response = await self.client.get(f"{self.base_url}/health")
        response.raise_for_status()
        return {
            "status": "healthy",
            "response_time_ms": response.elapsed.total_seconds() * 1000,
            "version": response.json().get("version")
        }
    
    async def check_database(self) -> Dict:
        """检查数据库连接"""
        response = await self.client.get(f"{self.base_url}/health/db")
        data = response.json()
        return {
            "status": "healthy" if data.get("connected") else "unhealthy",
            "connection_time_ms": data.get("connection_time_ms"),
            "active_connections": data.get("active_connections")
        }
    
    async def check_checkpoints(self) -> Dict:
        """检查检查点存储"""
        response = await self.client.get(f"{self.base_url}/health/checkpoints")
        data = response.json()
        return {
            "status": "healthy" if data.get("writable") else "unhealthy",
            "total_checkpoints": data.get("total"),
            "storage_used_mb": data.get("storage_used_mb"),
            "last_backup": data.get("last_backup")
        }
    
    async def check_graph(self) -> Dict:
        """检查LangGraph状态"""
        response = await self.client.post(
            f"{self.base_url}/health/graph",
            json={"test_query": "健康检查"}
        )
        data = response.json()
        return {
            "status": "healthy" if data.get("success") else "unhealthy",
            "execution_time_ms": data.get("execution_time_ms"),
            "nodes_available": data.get("nodes_available")
        }

async def main():
    checker = HealthChecker()
    
    try:
        health = await checker.check_health()
        
        print(json.dumps(health, indent=2, default=str))
        
        # 退出码
        if health["overall"]["status"] == "healthy":
            sys.exit(0)
        else:
            sys.exit(1)
            
    except Exception as e:
        print(f"健康检查失败: {e}", file=sys.stderr)
        sys.exit(2)

if __name__ == "__main__":
    asyncio.run(main())
```

### **性能基准测试脚本**
```python
# scripts/benchmark.py
import asyncio
import time
import statistics
from typing import List, Dict
import aiohttp
import pandas as pd
import matplotlib.pyplot as plt

class PerformanceBenchmark:
    def __init__(self, base_url: str, concurrent_users: int = 10):
        self.base_url = base_url
        self.concurrent_users = concurrent_users
        
    async def run_benchmark(self, test_cases: List[Dict]) -> pd.DataFrame:
        """运行性能基准测试"""
        results = []
        
        for test_case in test_cases:
            print(f"测试: {test_case['name']}")
            
            # 预热
            await self._warmup(test_case)
            
            # 并发测试
            latencies = await self._concurrent_test(test_case)
            
            # 计算统计信息
            stats = {
                "test_case": test_case["name"],
                "requests": len(latencies),
                "p50_ms": statistics.quantiles(latencies, n=100)[49],
                "p95_ms": statistics.quantiles(latencies, n=100)[94],
                "p99_ms": statistics.quantiles(latencies, n=100)[98],
                "avg_ms": statistics.mean(latencies),
                "min_ms": min(latencies),
                "max_ms": max(latencies),
                "success_rate": test_case.get("success_rate", 1.0)
            }
            
            results.append(stats)
            
            # 保存详细数据
            self._save_latency_distribution(test_case["name"], latencies)
        
        return pd.DataFrame(results)
    
    async def _concurrent_test(self, test_case: Dict) -> List[float]:
        """并发执行测试用例"""
        semaphore = asyncio.Semaphore(self.concurrent_users)
        
        async def make_request():
            async with semaphore:
                start_time = time.time()
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{self.base_url}{test_case['endpoint']}",
                            json=test_case["payload"],
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as response:
                            if response.status == 200:
                                return (time.time() - start_time) * 1000
                            else:
                                return None
                except Exception:
                    return None
        
        # 创建并发任务
        tasks = [make_request() for _ in range(test_case.get("requests", 100))]
        results = await asyncio.gather(*tasks)
        
        # 过滤失败的请求
        return [r for r in results if r is not None]
    
    def compare_with_baseline(self, current: pd.DataFrame, baseline: pd.DataFrame):
        """与基线比较"""
        comparison = pd.merge(
            current,
            baseline,
            on="test_case",
            suffixes=("_current", "_baseline")
        )
        
        # 计算差异百分比
        comparison["p95_change_pct"] = (
            (comparison["p95_ms_current"] - comparison["p95_ms_baseline"]) / 
            comparison["p95_ms_baseline"] * 100
        )
        
        comparison["success_rate_change"] = (
            comparison["success_rate_current"] - 
            comparison["success_rate_baseline"]
        )
        
        # 生成报告
        report = {
            "summary": {
                "total_tests": len(comparison),
                "improved": len(comparison[comparison["p95_change_pct"] < 0]),
                "regressed": len(comparison[comparison["p95_change_pct"] > 10]),
                "significant_regression": comparison[
                    comparison["p95_change_pct"] > 20
                ]["test_case"].tolist()
            },
            "details": comparison.to_dict("records")
        }
        
        # 生成可视化
        self._generate_charts(comparison)
        
        return report
```

## 5. 告警配置

### **Alertmanager配置**
```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: '${SMTP_PASSWORD}'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'slack-notifications'
  
  routes:
  - match:
      severity: critical
    receiver: 'pagerduty'
    
  - match:
      severity: warning
    receiver: