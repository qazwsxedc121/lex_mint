---
assistant_id: general-assistant
created_at: '2026-01-25T20:16:49.858653'
current_step: 1
model_id: deepseek:deepseek-chat
session_id: 460f69c4-8641-47f7-ae22-68d83c4196ca
title: LLM的流式输出是什么意思?
---

## User (2026-01-29 22:25:01)
LLM的流式输出是什么意思?
## Assistant (2026-01-29 22:25:15)
“流式输出”（Streaming Output）指的是大型语言模型（LLM）在生成文本时，**逐词或逐句实时返回结果**，而不是等待整个回答完全生成后再一次性输出。

### 主要特点：
1. **实时性**：用户可以看到文字逐个出现，类似打字效果，减少等待焦虑。
2. **低延迟**：生成开头内容后立即返回，无需等待全文生成完毕。
3. **节省时间**：对于长文本，用户可以边读边看，提前理解部分内容。

### 技术原理：
- 传统方式：模型生成完整回答后一次性返回。
- 流式输出：模型每生成一个词元（token）就立即输出，通过 HTTP 分块传输或 WebSocket 等技术实现持续推送。

### 常见应用场景：
- **聊天机器人**：对话回复实时显示。
- **代码生成**：逐步展示生成的代码。
- **长文本创作**：如文章、故事等逐步呈现。

### 优点：
- 用户体验更流畅，感知响应更快。
- 对于部分场景（如代码生成），用户可以提前中断或调整。

### 缺点：
- 可能增加网络请求次数。
- 若中间网络中断，可能需要重新生成。

### 示例：
- **非流式**：等待 5 秒后一次性显示完整段落。
- **流式**：立即开始显示“你好”，然后逐词出现“，我是 AI 助手……”。

目前大多数 LLM API（如 OpenAI、Claude 等）都支持流式输出，常用于需要实时交互的应用中。