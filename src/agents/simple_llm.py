"""ç®€å•çš„ LLM è°ƒç”¨æœåŠ¡ - ä¸ä½¿ç”¨ LangGraph"""

import os
import logging
from typing import List, Dict, Any, AsyncIterator, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from src.utils.llm_logger import get_llm_logger
from src.api.services.model_config_service import ModelConfigService

logger = logging.getLogger(__name__)


def call_llm(
    messages: List[Dict[str, str]],
    session_id: str = "unknown",
    model_id: Optional[str] = None
) -> str:
    """ç›´æ¥è°ƒç”¨ LLMï¼Œä¸ä½¿ç”¨ LangGraph.

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
        session_id: ä¼šè¯ IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model_id: æ¨¡å‹ IDï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹

    Returns:
        AI çš„å›å¤å†…å®¹
    """
    llm_logger = get_llm_logger()

    # åŠ¨æ€è·å– LLM å®ä¾‹
    model_service = ModelConfigService()
    llm = model_service.get_llm_instance(model_id)

    # è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹ ID
    actual_model_id = model_id or model_service.get_llm_instance().model_name

    print(f"ğŸ”§ å‡†å¤‡è°ƒç”¨ LLM (æ¨¡å‹: {actual_model_id})")
    print(f"   ä¼šè¯å†å²æ¶ˆæ¯æ•°: {len(messages)}")
    logger.info(f"ğŸ”§ å‡†å¤‡è°ƒç”¨ LLM (æ¨¡å‹: {actual_model_id})ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    langchain_messages = []
    for i, msg in enumerate(messages):
        if msg.get("role") == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: ç”¨æˆ· - {msg['content'][:50]}...")
        elif msg.get("role") == "assistant":
            langchain_messages.append(AIMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: åŠ©æ‰‹ - {msg['content'][:50]}...")

    try:
        print(f"ğŸš€ æ­£åœ¨å‘é€ {len(langchain_messages)} æ¡æ¶ˆæ¯åˆ° LLM API...")
        logger.info(f"ğŸš€ è°ƒç”¨ LLM API...")

        # è°ƒç”¨ LLMï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼ï¼‰
        response = llm.invoke(langchain_messages)

        print(f"âœ… æ”¶åˆ° LLM å›å¤ï¼Œé•¿åº¦: {len(response.content)} å­—ç¬¦")
        logger.info(f"âœ… æ”¶åˆ°å›å¤: {len(response.content)} å­—ç¬¦")

        # è®°å½•æ—¥å¿—
        llm_logger.log_interaction(
            session_id=session_id,
            messages_sent=langchain_messages,
            response_received=response,
            model=actual_model_id
        )
        print(f"ğŸ“ LLM äº¤äº’å·²è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶")

        return response.content

    except Exception as e:
        print(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {str(e)}")
        logger.error(f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        llm_logger.log_error(session_id, e, context="LLM API call")
        raise


async def call_llm_stream(
    messages: List[Dict[str, str]],
    session_id: str = "unknown",
    model_id: Optional[str] = None
) -> AsyncIterator[str]:
    """æµå¼è°ƒç”¨ LLMï¼Œé€tokenè¿”å›.

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
        session_id: ä¼šè¯ IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model_id: æ¨¡å‹ IDï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹

    Yields:
        AI å›å¤çš„æ¯ä¸ª token
    """
    llm_logger = get_llm_logger()

    # åŠ¨æ€è·å– LLM å®ä¾‹ï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰
    model_service = ModelConfigService()
    llm = model_service.get_llm_instance(model_id)
    # å¯ç”¨æµå¼è¾“å‡º
    llm.streaming = True

    # è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹ ID
    actual_model_id = model_id or model_service.get_llm_instance().model_name

    print(f"ğŸ”§ å‡†å¤‡æµå¼è°ƒç”¨ LLM (æ¨¡å‹: {actual_model_id})")
    print(f"   ä¼šè¯å†å²æ¶ˆæ¯æ•°: {len(messages)}")
    logger.info(f"ğŸ”§ å‡†å¤‡æµå¼è°ƒç”¨ LLM (æ¨¡å‹: {actual_model_id})ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    langchain_messages = []
    for i, msg in enumerate(messages):
        if msg.get("role") == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: ç”¨æˆ· - {msg['content'][:50]}...")
        elif msg.get("role") == "assistant":
            langchain_messages.append(AIMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: åŠ©æ‰‹ - {msg['content'][:50]}...")

    try:
        print(f"ğŸš€ æ­£åœ¨æµå¼å‘é€ {len(langchain_messages)} æ¡æ¶ˆæ¯åˆ° LLM API...")
        logger.info(f"ğŸš€ æµå¼è°ƒç”¨ LLM API...")

        # æ”¶é›†å®Œæ•´å›å¤ç”¨äºæ—¥å¿—è®°å½•
        full_response = ""

        # æµå¼è°ƒç”¨ LLM
        async for chunk in llm.astream(langchain_messages):
            if chunk.content:
                full_response += chunk.content
                yield chunk.content

        print(f"âœ… LLM æµå¼å›å¤å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")
        logger.info(f"âœ… æµå¼å›å¤å®Œæˆ: {len(full_response)} å­—ç¬¦")

        # è®°å½•å®Œæ•´äº¤äº’åˆ°æ—¥å¿—
        from langchain_core.messages import AIMessage as AIMsg
        response_msg = AIMsg(content=full_response)
        llm_logger.log_interaction(
            session_id=session_id,
            messages_sent=langchain_messages,
            response_received=response_msg,
            model=actual_model_id
        )
        print(f"ğŸ“ æµå¼ LLM äº¤äº’å·²è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶")

    except Exception as e:
        print(f"âŒ LLM æµå¼ API è°ƒç”¨å¤±è´¥: {str(e)}")
        logger.error(f"âŒ æµå¼ API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        llm_logger.log_error(session_id, e, context="LLM Stream API call")
        raise
