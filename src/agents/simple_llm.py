"""ç®€å•çš„ LLM è°ƒç”¨æœåŠ¡ - ä¸ä½¿ç”¨ LangGraph"""

import os
import logging
from typing import List, Dict, Any, AsyncIterator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from src.utils.llm_logger import get_llm_logger

logger = logging.getLogger(__name__)


def call_llm(messages: List[Dict[str, str]], session_id: str = "unknown") -> str:
    """ç›´æ¥è°ƒç”¨ LLMï¼Œä¸ä½¿ç”¨ LangGraph.

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
        session_id: ä¼šè¯ IDï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        AI çš„å›å¤å†…å®¹
    """
    llm_logger = get_llm_logger()

    print(f"ğŸ”§ å‡†å¤‡è°ƒç”¨ DeepSeek")
    print(f"   ä¼šè¯å†å²æ¶ˆæ¯æ•°: {len(messages)}")
    logger.info(f"ğŸ”§ å‡†å¤‡è°ƒç”¨ DeepSeekï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7,
        base_url="https://api.deepseek.com",
        api_key=os.getenv("DEEPSEEK_API_KEY")
    )

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    langchain_messages = []
    for i, msg in enumerate(messages):
        if msg.get("role") == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: ç”¨æˆ· - {msg['content'][:50]}...")
        elif msg.get("role") == "assistant":
            langchain_messages.append(AIMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: åŠ©æ‰‹ - {msg['content'][:50]}...")

    try:
        print(f"ğŸš€ æ­£åœ¨å‘é€ {len(langchain_messages)} æ¡æ¶ˆæ¯åˆ° DeepSeek API...")
        logger.info(f"ğŸš€ è°ƒç”¨ DeepSeek API...")

        # è°ƒç”¨ LLMï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼ï¼‰
        response = llm.invoke(langchain_messages)

        print(f"âœ… æ”¶åˆ° DeepSeek å›å¤ï¼Œé•¿åº¦: {len(response.content)} å­—ç¬¦")
        logger.info(f"âœ… æ”¶åˆ°å›å¤: {len(response.content)} å­—ç¬¦")

        # è®°å½•æ—¥å¿—
        llm_logger.log_interaction(
            session_id=session_id,
            messages_sent=langchain_messages,
            response_received=response,
            model="deepseek-chat"
        )
        print(f"ğŸ“ LLM äº¤äº’å·²è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶")

        return response.content

    except Exception as e:
        print(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}")
        logger.error(f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        llm_logger.log_error(session_id, e, context="LLM API call")
        raise


async def call_llm_stream(
    messages: List[Dict[str, str]],
    session_id: str = "unknown"
) -> AsyncIterator[str]:
    """æµå¼è°ƒç”¨ LLMï¼Œé€tokenè¿”å›.

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
        session_id: ä¼šè¯ IDï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Yields:
        AI å›å¤çš„æ¯ä¸ª token
    """
    llm_logger = get_llm_logger()

    print(f"ğŸ”§ å‡†å¤‡æµå¼è°ƒç”¨ DeepSeek")
    print(f"   ä¼šè¯å†å²æ¶ˆæ¯æ•°: {len(messages)}")
    logger.info(f"ğŸ”§ å‡†å¤‡æµå¼è°ƒç”¨ DeepSeekï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

    # åˆå§‹åŒ– LLM (streaming=True)
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7,
        base_url="https://api.deepseek.com",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        streaming=True  # å¯ç”¨æµå¼è¾“å‡º
    )

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    langchain_messages = []
    for i, msg in enumerate(messages):
        if msg.get("role") == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: ç”¨æˆ· - {msg['content'][:50]}...")
        elif msg.get("role") == "assistant":
            langchain_messages.append(AIMessage(content=msg["content"]))
            print(f"   æ¶ˆæ¯ {i+1}: åŠ©æ‰‹ - {msg['content'][:50]}...")

    try:
        print(f"ğŸš€ æ­£åœ¨æµå¼å‘é€ {len(langchain_messages)} æ¡æ¶ˆæ¯åˆ° DeepSeek API...")
        logger.info(f"ğŸš€ æµå¼è°ƒç”¨ DeepSeek API...")

        # æ”¶é›†å®Œæ•´å›å¤ç”¨äºæ—¥å¿—è®°å½•
        full_response = ""

        # æµå¼è°ƒç”¨ LLM
        async for chunk in llm.astream(langchain_messages):
            if chunk.content:
                full_response += chunk.content
                yield chunk.content

        print(f"âœ… DeepSeek æµå¼å›å¤å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")
        logger.info(f"âœ… æµå¼å›å¤å®Œæˆ: {len(full_response)} å­—ç¬¦")

        # è®°å½•å®Œæ•´äº¤äº’åˆ°æ—¥å¿—
        from langchain_core.messages import AIMessage as AIMsg
        response_msg = AIMsg(content=full_response)
        llm_logger.log_interaction(
            session_id=session_id,
            messages_sent=langchain_messages,
            response_received=response_msg,
            model="deepseek-chat"
        )
        print(f"ğŸ“ æµå¼ LLM äº¤äº’å·²è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶")

    except Exception as e:
        print(f"âŒ DeepSeek æµå¼ API è°ƒç”¨å¤±è´¥: {str(e)}")
        logger.error(f"âŒ æµå¼ API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        llm_logger.log_error(session_id, e, context="LLM Stream API call")
        raise
