"""Agent service for processing chat messages - ç®€åŒ–ç‰ˆï¼ˆä¸ä½¿ç”¨ LangGraphï¼‰"""

from typing import Dict, AsyncIterator
import logging
import asyncio

from src.agents.simple_llm import call_llm, call_llm_stream
from .conversation_storage import ConversationStorage

logger = logging.getLogger(__name__)


class AgentService:
    """Service layer for agent interactions with conversation storage.

    Coordinates the flow:
    1. Append user message to storage
    2. Load current conversation state
    3. Call LLM to generate response (ç›´æ¥è°ƒç”¨ï¼Œä¸ç”¨ LangGraph)
    4. Append assistant response to storage
    5. Return response to caller
    """

    def __init__(self, storage: ConversationStorage):
        """Initialize agent service.

        Args:
            storage: ConversationStorage instance for persistence
        """
        self.storage = storage
        logger.info("ğŸ¤– AgentService åˆå§‹åŒ–å®Œæˆï¼ˆç®€åŒ–ç‰ˆï¼‰")

    async def process_message(self, session_id: str, user_message: str) -> str:
        """Process a user message and return AI response.

        Args:
            session_id: Session UUID
            user_message: User's input text

        Returns:
            AI assistant's response text

        Raises:
            FileNotFoundError: If session doesn't exist
        """
        print(f"ğŸ“ [æ­¥éª¤ 1] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ–‡ä»¶...")
        logger.info(f"ğŸ“ [æ­¥éª¤ 1] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯")
        await self.storage.append_message(session_id, "user", user_message)
        print(f"âœ… ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜")

        print(f"ğŸ“‚ [æ­¥éª¤ 2] åŠ è½½ä¼šè¯çŠ¶æ€...")
        logger.info(f"ğŸ“‚ [æ­¥éª¤ 2] åŠ è½½ä¼šè¯çŠ¶æ€")
        session = await self.storage.get_session(session_id)
        messages = session["state"]["messages"]
        model_id = session.get("model_id")  # è·å–ä¼šè¯çš„æ¨¡å‹ ID
        print(f"âœ… ä¼šè¯åŠ è½½å®Œæˆï¼Œå½“å‰æœ‰ {len(messages)} æ¡æ¶ˆæ¯ï¼Œæ¨¡å‹: {model_id}")

        print(f"ğŸ§  [æ­¥éª¤ 3] è°ƒç”¨ LLM...")
        logger.info(f"ğŸ§  [æ­¥éª¤ 3] è°ƒç”¨ LLM")

        # ç›´æ¥è°ƒç”¨ LLMï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼ï¼‰ï¼Œä¼ é€’ model_id
        assistant_message = call_llm(messages, session_id=session_id, model_id=model_id)

        print(f"âœ… LLM å¤„ç†å®Œæˆ")
        logger.info(f"âœ… LLM å¤„ç†å®Œæˆ")
        print(f"ğŸ’¬ AI å›å¤é•¿åº¦: {len(assistant_message)} å­—ç¬¦")

        print(f"ğŸ“ [æ­¥éª¤ 4] ä¿å­˜ AI å›å¤åˆ°æ–‡ä»¶...")
        logger.info(f"ğŸ“ [æ­¥éª¤ 4] ä¿å­˜ AI å›å¤")
        await self.storage.append_message(session_id, "assistant", assistant_message)
        print(f"âœ… AI å›å¤å·²ä¿å­˜")

        return assistant_message

    async def process_message_stream(
        self,
        session_id: str,
        user_message: str,
        skip_user_append: bool = False
    ) -> AsyncIterator[str]:
        """æµå¼å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å› AI å“åº”æµ.

        Args:
            session_id: Session UUID
            user_message: User's input text
            skip_user_append: æ˜¯å¦è·³è¿‡è¿½åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆé‡æ–°ç”Ÿæˆæ—¶ä½¿ç”¨ï¼‰

        Yields:
            AI assistant's response tokens

        Raises:
            FileNotFoundError: If session doesn't exist
        """
        # ä»…å½“ skip_user_append=False æ—¶è¿½åŠ ç”¨æˆ·æ¶ˆæ¯
        if not skip_user_append:
            print(f"ğŸ“ [æ­¥éª¤ 1] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ–‡ä»¶...")
            logger.info(f"ğŸ“ [æ­¥éª¤ 1] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯")
            await self.storage.append_message(session_id, "user", user_message)
            print(f"âœ… ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜")
        else:
            print(f"â­ï¸ [æ­¥éª¤ 1] è·³è¿‡ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆé‡æ–°ç”Ÿæˆæ¨¡å¼ï¼‰")
            logger.info(f"â­ï¸ [æ­¥éª¤ 1] è·³è¿‡ä¿å­˜ç”¨æˆ·æ¶ˆæ¯")

        print(f"ğŸ“‚ [æ­¥éª¤ 2] åŠ è½½ä¼šè¯çŠ¶æ€...")
        logger.info(f"ğŸ“‚ [æ­¥éª¤ 2] åŠ è½½ä¼šè¯çŠ¶æ€")
        session = await self.storage.get_session(session_id)
        messages = session["state"]["messages"]
        assistant_id = session.get("assistant_id")
        model_id = session.get("model_id")
        print(f"âœ… ä¼šè¯åŠ è½½å®Œæˆï¼Œå½“å‰æœ‰ {len(messages)} æ¡æ¶ˆæ¯")
        print(f"   åŠ©æ‰‹ID: {assistant_id}, æ¨¡å‹: {model_id}")

        # è·å–åŠ©æ‰‹é…ç½®ï¼ˆåŒ…æ‹¬ç³»ç»Ÿæç¤ºè¯å’Œæœ€å¤§å¯¹è¯è½®æ•°ï¼‰
        system_prompt = None
        max_rounds = None

        # æ£€æŸ¥æ˜¯å¦æ˜¯ legacy ä¼šè¯æ ‡è¯†
        if assistant_id and assistant_id.startswith("__legacy_model_"):
            # æ—§ä¼šè¯ï¼šåªä½¿ç”¨ model_idï¼Œä¸ä½¿ç”¨åŠ©æ‰‹é…ç½®
            print(f"   ä½¿ç”¨æ—§ä¼šè¯æ¨¡å¼ï¼ˆä»…æ¨¡å‹ï¼‰")
        elif assistant_id:
            # æ–°ä¼šè¯ï¼šä»åŠ©æ‰‹é…ç½®åŠ è½½ç³»ç»Ÿæç¤ºè¯å’Œå¯¹è¯è½®æ•°é™åˆ¶
            from .assistant_config_service import AssistantConfigService
            assistant_service = AssistantConfigService()
            try:
                assistant = await assistant_service.get_assistant(assistant_id)
                if assistant:
                    system_prompt = assistant.system_prompt
                    max_rounds = assistant.max_rounds
                    print(f"   ä½¿ç”¨åŠ©æ‰‹é…ç½®:")
                    if system_prompt:
                        print(f"     - ç³»ç»Ÿæç¤ºè¯: {system_prompt[:50]}...")
                    if max_rounds:
                        if max_rounds == -1:
                            print(f"     - å¯¹è¯è½®æ•°: æ— é™åˆ¶")
                        else:
                            print(f"     - æœ€å¤§è½®æ•°: {max_rounds}")
            except Exception as e:
                logger.warning(f"   åŠ è½½åŠ©æ‰‹é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

        print(f"ğŸ§  [æ­¥éª¤ 3] æµå¼è°ƒç”¨ LLM...")
        logger.info(f"ğŸ§  [æ­¥éª¤ 3] æµå¼è°ƒç”¨ LLM")

        # æ”¶é›†å®Œæ•´å›å¤ç”¨äºä¿å­˜
        full_response = ""

        try:
            # æµå¼è°ƒç”¨ LLMï¼Œä¼ é€’ model_idã€system_prompt å’Œ max_rounds
            async for chunk in call_llm_stream(
                messages,
                session_id=session_id,
                model_id=model_id,
                system_prompt=system_prompt,
                max_rounds=max_rounds
            ):
                full_response += chunk
                yield chunk

            print(f"âœ… LLM æµå¼å¤„ç†å®Œæˆ")
            logger.info(f"âœ… LLM æµå¼å¤„ç†å®Œæˆ")
            print(f"ğŸ’¬ AI å›å¤æ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")

        except asyncio.CancelledError:
            # æµå¼ä¸­æ­¢ï¼Œä¿å­˜éƒ¨åˆ†å†…å®¹
            print(f"âš ï¸ æµå¼ç”Ÿæˆè¢«ä¸­æ­¢ï¼Œä¿å­˜éƒ¨åˆ†å†…å®¹...")
            logger.warning(f"âš ï¸ æµå¼ç”Ÿæˆè¢«ä¸­æ­¢ï¼Œä¿å­˜éƒ¨åˆ†å†…å®¹ï¼ˆ{len(full_response)} å­—ç¬¦ï¼‰")
            if full_response:
                await self.storage.append_message(session_id, "assistant", full_response)
                print(f"âœ… éƒ¨åˆ† AI å›å¤å·²ä¿å­˜")
            raise

        print(f"ğŸ“ [æ­¥éª¤ 4] ä¿å­˜å®Œæ•´ AI å›å¤åˆ°æ–‡ä»¶...")
        logger.info(f"ğŸ“ [æ­¥éª¤ 4] ä¿å­˜å®Œæ•´ AI å›å¤")
        await self.storage.append_message(session_id, "assistant", full_response)
        print(f"âœ… AI å›å¤å·²ä¿å­˜")
